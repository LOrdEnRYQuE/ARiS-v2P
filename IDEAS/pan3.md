Excellent point. The initial plan lays out the "what" and "when," but we need to explicitly define the "how" behind the agents' intelligence. Integrating these core AI concepts is what will make ARiS truly powerful.

Let's amend the project plan to include a formal **Core AI Strategy** section. This will detail how and where these technologies will be implemented across the project phases.

---

### **Amended Project Plan: ARiS - Core AI Strategy**

This section details the key machine learning and AI methodologies that will power the ARiS agent ecosystem.

#### **1. RAG (Retrieval-Augmented Generation)**

* **Role in ARiS:** RAG is our foundational strategy for providing agents with real-time, context-specific knowledge. It prevents agents from "hallucinating" and ensures their output is grounded in the reality of the user's project and relevant technologies. It is the core of our "long-term memory" system.
* **Integration Plan:**
    * **Phase 1-2:** The RAG pipeline will be built as a core service. The `Supabase pg_vector` store will be our vector database.
    * **Data Sources:** The RAG system will retrieve information from multiple sources:
        1.  **The User's Workspace (Live):** The entire codebase of the user's current project will be parsed, chunked by function/class using Tree-sitter, embedded, and stored. This is the primary context source.
        2.  **Technical Documentation (Cached):** We will pre-process and embed the official documentation for major frameworks (e.g., React, Node.js, Express, Next.js). When `Scriba-Frontend` is asked to build a React component, it will first retrieve relevant examples from the official React docs.
        3.  **Best Practices Corpus (Curated):** We will maintain a curated library of high-quality, vetted code snippets representing ideal design patterns. `Architectus` will use this to inform its structural decisions.
    * **Agent Usage:** Nearly every agent will use RAG. `Genesis` will use it to understand ambiguous terms, `Architectus` will use it to research design patterns, and `Scriba` agents will use it to find code examples before they begin writing.

#### **2. Fine-Tuning**

* **Role in ARiS:** While base models like GPT-4o are excellent generalists, fine-tuning will allow us to create hyper-specialized "expert" agents that are faster, cheaper, and more accurate for specific, repetitive tasks. This is our path to scaling effectively.
* **Integration Plan:**
    * **Phase 1-3 (Data Collection):** We will not begin fine-tuning immediately. Instead, we will collect high-quality data. Every code block generated by a `Scriba` agent that is approved by the `Auditor` and ultimately accepted by the user will be saved as a high-quality "instruction-response" pair. User corrections are even more valuable data.
    * **Phase 4 (Initial Fine-Tuning):** We will use our collected dataset to fine-tune our first specialist model. For example, we'll take an open-source model (like Code Llama or a Mistral model) and fine-tune it specifically on our dataset of approved React components. This will create `Scriba-Frontend-React-v1-tuned`.
    * **Phase 5+ (Ongoing Specialization):** This becomes a continuous improvement cycle. As we gather more data, we can create more experts (`Scriba-Backend-Express-tuned`, `Structor-DB-Postgres-tuned`, etc.), offloading more work from expensive generalist models to our cheap, efficient specialists.

#### **3. Advanced Context Management (Our "Multi-Context Processing" System)**

* **Role in ARiS:** To achieve true intelligence, the system needs to understand more than just the text of a single file. Our MCP system will create a rich, holistic state representation of the "developer's world" at any given moment.
* **Integration Plan:** Context will be layered and become richer with each phase:
    * **Phase 1:** Context = File content + Project file tree.
    * **Phase 2:** Context += Code structure (ASTs from Tree-sitter). The agent now understands the difference between a function and a comment.
    * **Phase 3:** Context += Conversational history from `Genesis`. The agent knows what was discussed previously.
    * **Phase 4:** Context += Terminal state (last command, output, errors), dependency graph (`package.json`), and live IDE state (user's cursor position, selected text).
    * **The Goal:** Before `Prometheus` dispatches a task, it assembles a complete context package from all these sources. When a user says, "fix this," the agent will know what "this" refers to based on their cursor, the errors in the terminal, and the code's structure.

#### **4. Intelligent Agentic Loops (Self-Correction)**

* **Role in ARiS:** This is what makes the system "intelligent" rather than just a script. It's the ability for the agent society to recognize failure, learn from it, and try again without user intervention.
* **Integration Plan:** This is the core logic of the `Prometheus` <> `Scriba` <> `Auditor` loop, which begins in **Phase 2**.
    * **The Self-Correction Loop:**
        1.  `Prometheus` gives a task to `Scriba`: "Write the user login function."
        2.  `Scriba` generates the code.
        3.  The code is passed to `Auditor`.
        4.  `Auditor` does more than just read. It runs a linter, a type-checker (like TypeScript), and attempts a test compilation in a sandboxed environment.
        5.  **Failure Case:** The compilation fails with a "Type 'any' is not assignable to type 'string'" error. `Auditor` does not just reject the code; it creates an "Error Report."
        6.  `Prometheus` receives the failed task and the Error Report.
        7.  `Prometheus` re-issues the task to `Scriba`, but with new instructions: "Your last attempt to write the user login function failed. Fix the following error: `Type 'any' is not assignable to type 'string'`."
        8.  This loop repeats up to N times or until the code passes the `Auditor`'s checks.

By formally integrating these four pillars, we ensure that ARiS is not just a code generator but a learning, adapting, and self-correcting system. This is the path to building a truly powerful and reliable tool.
